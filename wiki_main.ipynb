{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdKxR_24FZ4S"
      },
      "outputs": [],
      "source": [
        "!pip install wikipedia-api newspaper3k pandas nltk transformers torch scikit-learn tqdm regex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIsKz_9XFaKU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "from newspaper import Article\n",
        "import wikipediaapi\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import joblib\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import re\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnpJcuzmVHHz"
      },
      "outputs": [],
      "source": [
        "def clean_sentence(sentence):\n",
        "    \"\"\"\n",
        "    Cleans the input sentence by removing footnote markers, newlines, and extra spaces.\n",
        "    Ensures the sentence is concise.\n",
        "    \"\"\"\n",
        "    # Remove footnote markers like [1], [2], etc.\n",
        "    sentence = re.sub(r'\\[\\d+\\]', '', sentence)\n",
        "    # Remove newlines and replace multiple spaces with a single space\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "    # Strip leading/trailing spaces\n",
        "    sentence = sentence.strip()\n",
        "    # Exclude very short sentences\n",
        "    if len(sentence) < 20:\n",
        "        return ''\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlpaGl_AFfta"
      },
      "outputs": [],
      "source": [
        "# Check for CUDA availability\n",
        "device = 0 if torch.cuda.is_available() else -1  # Use GPU if available\n",
        "\n",
        "# Initialize Hugging Face pipelines\n",
        "# 1. Claim Detection - Using Zero-Shot Classification\n",
        "claim_detection_pipeline = pipeline(\n",
        "    \"zero-shot-classification\",\n",
        "    model=\"facebook/bart-large-mnli\",\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# 2. Fact Verification - Using DeBERTa model fine-tuned on FEVER\n",
        "fact_verification_pipeline = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"learn3r/deberta-v3-base-finetuned-fever\",\n",
        "    return_all_scores=True,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qX0XUsG95xh"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "SCIENCE_CATEGORIES = [\n",
        "    # \"Physics\",\n",
        "    # \"Chemistry\",\n",
        "    # \"Biology\",\n",
        "    # \"Astronomy\",\n",
        "    # \"Earth science\",\n",
        "    # \"Computer science\",\n",
        "    # \"Mathematics\",\n",
        "    # \"Engineering\",\n",
        "    \"Environmental science\",\n",
        "    # \"Medicine\"\n",
        "]\n",
        "WIKI_LANGUAGE = 'en'\n",
        "MAX_ARTICLES_PER_CATEGORY = 5\n",
        "MAX_SENTENCE_LENGTH = 200  # Maximum characters per claim\n",
        "\n",
        "# Domain Keywords\n",
        "DOMAIN_KEYWORDS = {\n",
        "    # 'Physics': ['physics', 'quantum', 'relativity', 'energy'],\n",
        "    # 'Chemistry': ['chemistry', 'molecule', 'reaction', 'compound'],\n",
        "    # 'Biology': ['biology', 'cell', 'gene', 'ecosystem'],\n",
        "    # 'Astronomy': ['astronomy', 'galaxy', 'star', 'planet'],\n",
        "    # 'Earth science': ['earth', 'geology', 'climate', 'soil'],\n",
        "    # 'Computer science': ['computer', 'algorithm', 'software', 'hardware'],\n",
        "    # 'Mathematics': ['mathematics', 'calculus', 'algebra', 'geometry'],\n",
        "    # 'Engineering': ['engineering', 'mechanical', 'electrical', 'civil'],\n",
        "    'Environmental science': ['environment', 'sustainability', 'conservation', 'pollution'],\n",
        "    # 'Medicine': ['medicine', 'health', 'disease', 'therapy']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4iyeutMFgE1"
      },
      "outputs": [],
      "source": [
        "class WikipediaScraper:\n",
        "    def __init__(self, language=WIKI_LANGUAGE, user_agent='MyFactCheckTool/1.0'):\n",
        "        self.wiki = wikipediaapi.Wikipedia(\n",
        "            language=language,\n",
        "            extract_format=wikipediaapi.ExtractFormat.WIKI,\n",
        "            user_agent=user_agent\n",
        "        )\n",
        "\n",
        "    def get_science_articles(self, categories, max_articles=10):\n",
        "        \"\"\"\n",
        "        Fetches article URLs from the given categories.\n",
        "        \"\"\"\n",
        "        article_urls = []\n",
        "        for category in tqdm(categories, desc=\"Fetching Categories\"):\n",
        "            cat = self.wiki.page(f'Category:{category}')\n",
        "            if not cat.exists():\n",
        "                print(f\"Category '{category}' does not exist.\")\n",
        "                continue\n",
        "            articles = self._get_articles_from_category(cat, max_articles)\n",
        "            for article in articles:\n",
        "                article_urls.append(article.fullurl)\n",
        "        print(f\"Fetched a total of {len(article_urls)} articles from Wikipedia.\")\n",
        "        return article_urls\n",
        "\n",
        "    def _get_articles_from_category(self, category_page, max_articles):\n",
        "        \"\"\"\n",
        "        Retrieves articles from a specific category page.\n",
        "        \"\"\"\n",
        "        articles = []\n",
        "        for c in category_page.categorymembers.values():\n",
        "            if c.ns == wikipediaapi.Namespace.MAIN and len(articles) < max_articles:\n",
        "                articles.append(c)\n",
        "        return articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQm9k85zFhX0"
      },
      "outputs": [],
      "source": [
        "class WebScraper:\n",
        "    def __init__(self, urls):\n",
        "        self.urls = urls\n",
        "\n",
        "    def extract_statements(self):\n",
        "        \"\"\"\n",
        "        Extracts factual statements from the list of URLs.\n",
        "        \"\"\"\n",
        "        statements = []\n",
        "        for url in tqdm(self.urls, desc=\"Scraping Articles\"):\n",
        "            try:\n",
        "                article = Article(url)\n",
        "                article.download()\n",
        "                article.parse()\n",
        "                paragraphs = sent_tokenize(article.text)\n",
        "                factual_claims = self.extract_factual_claims(paragraphs)\n",
        "                for claim in factual_claims:\n",
        "                    cleaned_claim = clean_sentence(claim)\n",
        "                    if cleaned_claim:  # Ensure the claim is not empty after cleaning\n",
        "                        statements.append({'Statement': cleaned_claim, 'Source': url})\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {url}: {e}\")\n",
        "        print(f\"Extracted a total of {len(statements)} factual statements from articles.\")\n",
        "        return statements\n",
        "\n",
        "    def extract_factual_claims(self, paragraphs):\n",
        "        \"\"\"\n",
        "        Identifies factual claims within paragraphs.\n",
        "        \"\"\"\n",
        "        factual_claims = []\n",
        "        for paragraph in paragraphs:\n",
        "            sentences = sent_tokenize(paragraph)\n",
        "            for sentence in sentences:\n",
        "                if self.is_factual(sentence):\n",
        "                    factual_claims.append(sentence)\n",
        "        return factual_claims\n",
        "\n",
        "    def is_factual(self, sentence):\n",
        "        \"\"\"\n",
        "        Determines if a sentence is a factual claim using the claim detection pipeline.\n",
        "        \"\"\"\n",
        "        if len(sentence) > MAX_SENTENCE_LENGTH:\n",
        "            return False  # Skip sentences that are too long\n",
        "        candidate_labels = ['factual', 'opinion']\n",
        "        try:\n",
        "            result = claim_detection_pipeline(sentence, candidate_labels)\n",
        "            # Check if the highest scored label is 'factual' and score > 0.5\n",
        "            return result['labels'][0] == 'factual' and result['scores'][0] > 0.5\n",
        "        except Exception as e:\n",
        "            print(f\"Error in claim detection: {e}\")\n",
        "            return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ALo-obVFixZ"
      },
      "outputs": [],
      "source": [
        "class FactChecker:\n",
        "    def __init__(self, pipeline):\n",
        "        \"\"\"\n",
        "        Initializes the FactChecker with a Hugging Face fact verification pipeline.\n",
        "        \"\"\"\n",
        "        self.pipeline = pipeline\n",
        "\n",
        "    def verify_statement(self, statement):\n",
        "        \"\"\"\n",
        "        Verifies the statement's veracity ('True', 'False', 'Unknown') using the fact verification pipeline.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            result = self.pipeline(statement)\n",
        "            # The model returns scores for 'LABEL_0', 'LABEL_1', 'LABEL_2'\n",
        "            # Typically, 'LABEL_0' = 'NOT ENOUGH INFO', 'LABEL_1' = 'SUPPORTS', 'LABEL_2' = 'REFUTES'\n",
        "            if not result:\n",
        "                return 'Unknown'\n",
        "\n",
        "            labels = result[0]\n",
        "            top_label = labels[0]['label']\n",
        "            return [x['score'] for x in labels]\n",
        "        except Exception as e:\n",
        "            print(f\"Error in fact verification: {e}\")\n",
        "            return 'Unknown'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYTr3kwkFk41"
      },
      "outputs": [],
      "source": [
        "class DomainClassifier:\n",
        "    def __init__(self, keywords=DOMAIN_KEYWORDS):\n",
        "        self.keywords = keywords\n",
        "\n",
        "    def classify_domain(self, statement):\n",
        "        statement_lower = statement.lower()\n",
        "        for domain, kws in self.keywords.items():\n",
        "            for kw in kws:\n",
        "                if kw in statement_lower:\n",
        "                    return domain\n",
        "        return 'Unknown'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuFv0GvPFmm2"
      },
      "outputs": [],
      "source": [
        "# Initialize Scrapers and Checkers\n",
        "wiki_scraper = WikipediaScraper()\n",
        "\n",
        "\n",
        "# Fetch Article URLs\n",
        "article_urls = wiki_scraper.get_science_articles(SCIENCE_CATEGORIES, MAX_ARTICLES_PER_CATEGORY)\n",
        "\n",
        "# Scrape Articles and Extract Statements\n",
        "web_scraper = WebScraper(article_urls)\n",
        "statements = web_scraper.extract_statements()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Afjo2SRD8g0S"
      },
      "outputs": [],
      "source": [
        "def get_label(probabilities):\n",
        "    return probabilities[0] > 0.7 and probabilities[1]<0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_n-yK9nu6-Ot"
      },
      "outputs": [],
      "source": [
        "fact_checker = FactChecker(fact_verification_pipeline)\n",
        "domain_classifier = DomainClassifier()  # Ensure the model is trained\n",
        "# Process Statements: Fact-Check and Domain Classification\n",
        "for stmt in tqdm(statements, desc=\"Processing Statements\"):\n",
        "    statement_text = stmt['Statement']\n",
        "    source_url = stmt['Source']\n",
        "\n",
        "    # Fact-Check the Statement\n",
        "    label = fact_checker.verify_statement(statement_text)\n",
        "\n",
        "    # Classify the Domain\n",
        "    domain = domain_classifier.classify_domain(statement_text)\n",
        "\n",
        "    # Update the statement with label and domain\n",
        "    stmt['Label'] = label\n",
        "    stmt['Domain'] = domain\n",
        "\n",
        "# Create DataFram\n",
        "df = pd.DataFrame(statements, columns=['Statement', 'Label', 'Domain', 'Source'])\n",
        "df['Label'] = df['Label'].apply(get_label)\n",
        "df = df[df['Domain']!='Unknown']\n",
        "# give claims that start with Capital letter\n",
        "df = df[df['Statement'].str[0].str.isupper()]\n",
        "# shuffle\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "# Display the DataFrame\n",
        "df.head(50)\n",
        "\n",
        "# save to excel\n",
        "df.to_excel('statements.xlsx', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
